{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, you will generate names using recurrent neural networks!\n",
    "\n",
    "The used dataset is in the file `names.txt`, a file encoded in `'ISO-8859-1'`, containing more than 10 000 names.\n",
    "\n",
    "First load it, and have a look at the names, and clean the dataset if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaliyah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapeli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aapo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aarne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name\n",
       "0  aaliyah\n",
       "1   aapeli\n",
       "2     aapo\n",
       "3    aaren\n",
       "4    aarne"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset and explore it\n",
    "import pandas as pd\n",
    "names=pd.read_csv('names.txt', encoding=\"ISO-8859-1\")\n",
    "\n",
    "names = names.drop_duplicates()\n",
    "\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN needs to understand where is the beginning and the end of a word. So we need to add a new character at the beginning of every word, for example `'\\t'` (it could be anything else as long as it can be identified easily). We can also add `'\\n'` to the end of every word as the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add '\\t' at the beginning of every word\n",
    "names['name'] =names[\"name\"].apply(lambda x: '\\t'+str(x)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate names, we will have to play at the character level: we will train a RNN to predict the next character, knowing the previous one. So, compute a list of all the possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 55\n",
      "{'ú', 'o', 'ö', 'ù', 't', 'å', 'è', 'n', 'ã', 'ê', 's', '\\t', 'l', 'ð', 'b', 'à', 'f', 'p', 'r', 'a', 'g', '\\n', 'ì', 'ä', 'æ', 'v', 'j', 'ï', 'ü', 'm', 'é', 'k', 'h', 'x', 'c', 'ø', 'e', 'ç', 'q', 'w', 'ñ', 'á', 'i', 'í', 'õ', 'u', 'd', 'þ', 'y', 'ò', 'z', 'ë', 'ô', '-', 'ó'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute and display the list of all possible characters\n",
    "# Get the vocab dict\n",
    "all_chars=set()\n",
    "for name in names.name:\n",
    "    for c in name:\n",
    "        if c not in all_chars:\n",
    "            all_chars.add(c)\n",
    "all_chars.add('\\n')\n",
    "\n",
    "print('number of characters', len(all_chars))\n",
    "print(all_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 55 characters, including the newly added `\\t` and `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual when playing with characters (or words), we will convert them into integers. So build a dictionary `char_to_idx` that, given a character as key, returns an integer. And build the opposite dictionary `idx_to_char` that, given an integer as key, returns the corresponding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " '-': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'f': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'i': 11,\n",
       " 'j': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'q': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " 'à': 29,\n",
       " 'á': 30,\n",
       " 'ã': 31,\n",
       " 'ä': 32,\n",
       " 'å': 33,\n",
       " 'æ': 34,\n",
       " 'ç': 35,\n",
       " 'è': 36,\n",
       " 'é': 37,\n",
       " 'ê': 38,\n",
       " 'ë': 39,\n",
       " 'ì': 40,\n",
       " 'í': 41,\n",
       " 'ï': 42,\n",
       " 'ð': 43,\n",
       " 'ñ': 44,\n",
       " 'ò': 45,\n",
       " 'ó': 46,\n",
       " 'ô': 47,\n",
       " 'õ': 48,\n",
       " 'ö': 49,\n",
       " 'ø': 50,\n",
       " 'ù': 51,\n",
       " 'ú': 52,\n",
       " 'ü': 53,\n",
       " 'þ': 54}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the idx_to_char and char_to_idx dict\n",
    "char_to_idx = { ch:i for i,ch in enumerate(sorted(all_chars)) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(sorted(all_chars)) }\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into the neural network part, we have one more step: **create the X and y data**!\n",
    "\n",
    "So the **X** data is going to be, for every name, all but the `'\\n'` character. The **y** data will be all but the `'\\t'` character.\n",
    "\n",
    "Indeed, we will try to predict the following character knowing the previous. To the **X** does not need the final character, and the **y** does not need the first character.\n",
    "\n",
    "Create the columns X and y to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\taaliyah\\n</td>\n",
       "      <td>\\taaliyah</td>\n",
       "      <td>aaliyah\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\taapeli\\n</td>\n",
       "      <td>\\taapeli</td>\n",
       "      <td>aapeli\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\taapo\\n</td>\n",
       "      <td>\\taapo</td>\n",
       "      <td>aapo\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\taaren\\n</td>\n",
       "      <td>\\taaren</td>\n",
       "      <td>aaren\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\taarne\\n</td>\n",
       "      <td>\\taarne</td>\n",
       "      <td>aarne\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name          X          y\n",
       "0  \\taaliyah\\n  \\taaliyah  aaliyah\\n\n",
       "1   \\taapeli\\n   \\taapeli   aapeli\\n\n",
       "2     \\taapo\\n     \\taapo     aapo\\n\n",
       "3    \\taaren\\n    \\taaren    aaren\\n\n",
       "4    \\taarne\\n    \\taarne    aarne\\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create the columns X and y\n",
    "names['X'] = names[\"name\"].apply(lambda x: x[:len(x)-1])\n",
    "names['y'] = names[\"name\"].apply(lambda x: x[1:len(x)])\n",
    "names.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using your `char_to_idx` dict, compute the corresponding `X` and `y` containing, for each name, a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the X and y variables containing integers only\n",
    "X = names['X'].apply(lambda x: [char_to_idx[c] for c in x])\n",
    "y = names['y'].apply(lambda x: [char_to_idx[c] for c in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was complicated, but are now in a known case, use keras and `pad_sequence()` function to get a proper `X` and `y` variables with a `maxlen=16` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11497, 16), (11497, 16))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Use pad_sequences to get only sequences of length 16 for each name\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "maxlen = 16\n",
    "\n",
    "X_train = sequence.pad_sequences(X,\n",
    "                                 value=0,\n",
    "                                 padding='post',\n",
    "                                 maxlen=maxlen)\n",
    "\n",
    "y_train = sequence.pad_sequences(y,\n",
    "                                 value=0,\n",
    "                                 padding='post',\n",
    "                                 maxlen=maxlen)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the function `to_categorical()`, make the one-hot-encoding needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11497, 16, 55), (11497, 16, 55))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use to_categorical to perform one hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = to_categorical(X_train)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should finally have arrays of shape `(number of names, 16, 55)`:\n",
    "- `16` is the sequence length if you chose `maxlen=16`\n",
    "- `55` is the number of possible characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to build and train a neural network. You can for example use one or two layers of GRU (or LSTM). Do not forget to set `return_sequences=True`. \n",
    "\n",
    "Then you will have to add a `TimeDistributed(Dense(55))` with a softmax activation function. This layer will handle the fact you have a dense layer at each time step with a softmax prediction of the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the neural network and train it\n",
    "from tensorflow.keras.layers import GRU, Dense, TimeDistributed, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(input_dim=55, input_length=16, output_dim=32))\n",
    "model.add(GRU(32, input_shape=(maxlen, len(all_chars)), return_sequences=True))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "#model.add(GRU(32, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(all_chars), activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11497 samples\n",
      "Epoch 1/50\n",
      "11497/11497 [==============================] - 5s 476us/sample - loss: 2.0332\n",
      "Epoch 2/50\n",
      "11497/11497 [==============================] - 3s 246us/sample - loss: 1.4019\n",
      "Epoch 3/50\n",
      "11497/11497 [==============================] - 3s 263us/sample - loss: 1.2590\n",
      "Epoch 4/50\n",
      "11497/11497 [==============================] - 3s 272us/sample - loss: 1.1859\n",
      "Epoch 5/50\n",
      "11497/11497 [==============================] - 3s 277us/sample - loss: 1.1544\n",
      "Epoch 6/50\n",
      "11497/11497 [==============================] - 3s 241us/sample - loss: 1.1372\n",
      "Epoch 7/50\n",
      "11497/11497 [==============================] - 3s 273us/sample - loss: 1.1260\n",
      "Epoch 8/50\n",
      "11497/11497 [==============================] - 3s 272us/sample - loss: 1.1168\n",
      "Epoch 9/50\n",
      "11497/11497 [==============================] - 3s 283us/sample - loss: 1.1104\n",
      "Epoch 10/50\n",
      "11497/11497 [==============================] - 3s 274us/sample - loss: 1.1043\n",
      "Epoch 11/50\n",
      "11497/11497 [==============================] - 3s 242us/sample - loss: 1.0993\n",
      "Epoch 12/50\n",
      "11497/11497 [==============================] - 3s 276us/sample - loss: 1.0943\n",
      "Epoch 13/50\n",
      "11497/11497 [==============================] - 3s 279us/sample - loss: 1.0898\n",
      "Epoch 14/50\n",
      "11497/11497 [==============================] - 3s 282us/sample - loss: 1.0853\n",
      "Epoch 15/50\n",
      "11497/11497 [==============================] - 3s 280us/sample - loss: 1.0815\n",
      "Epoch 16/50\n",
      "11497/11497 [==============================] - 3s 269us/sample - loss: 1.0777\n",
      "Epoch 17/50\n",
      "11497/11497 [==============================] - 3s 282us/sample - loss: 1.0740\n",
      "Epoch 18/50\n",
      "11497/11497 [==============================] - 3s 263us/sample - loss: 1.0706\n",
      "Epoch 19/50\n",
      "11497/11497 [==============================] - 3s 231us/sample - loss: 1.0673\n",
      "Epoch 20/50\n",
      "11497/11497 [==============================] - 3s 275us/sample - loss: 1.0641\n",
      "Epoch 21/50\n",
      "11497/11497 [==============================] - 3s 257us/sample - loss: 1.0612\n",
      "Epoch 22/50\n",
      "11497/11497 [==============================] - 3s 259us/sample - loss: 1.0581\n",
      "Epoch 23/50\n",
      "11497/11497 [==============================] - 3s 273us/sample - loss: 1.0554\n",
      "Epoch 24/50\n",
      "11497/11497 [==============================] - 3s 279us/sample - loss: 1.0528\n",
      "Epoch 25/50\n",
      "11497/11497 [==============================] - 3s 276us/sample - loss: 1.0504\n",
      "Epoch 26/50\n",
      "11497/11497 [==============================] - 3s 248us/sample - loss: 1.0481\n",
      "Epoch 27/50\n",
      "11497/11497 [==============================] - 3s 284us/sample - loss: 1.0462\n",
      "Epoch 28/50\n",
      "11497/11497 [==============================] - 3s 287us/sample - loss: 1.0440\n",
      "Epoch 29/50\n",
      "11497/11497 [==============================] - 3s 277us/sample - loss: 1.0419\n",
      "Epoch 30/50\n",
      "11497/11497 [==============================] - 3s 279us/sample - loss: 1.0399\n",
      "Epoch 31/50\n",
      "11497/11497 [==============================] - 3s 248us/sample - loss: 1.0379\n",
      "Epoch 32/50\n",
      "11497/11497 [==============================] - 3s 276us/sample - loss: 1.0361\n",
      "Epoch 33/50\n",
      "11497/11497 [==============================] - 3s 272us/sample - loss: 1.0339\n",
      "Epoch 34/50\n",
      "11497/11497 [==============================] - 3s 254us/sample - loss: 1.0323\n",
      "Epoch 35/50\n",
      "11497/11497 [==============================] - 3s 256us/sample - loss: 1.0305\n",
      "Epoch 36/50\n",
      "11497/11497 [==============================] - 2s 204us/sample - loss: 1.0290\n",
      "Epoch 37/50\n",
      "11497/11497 [==============================] - 2s 217us/sample - loss: 1.0269\n",
      "Epoch 38/50\n",
      "11497/11497 [==============================] - 3s 288us/sample - loss: 1.0250\n",
      "Epoch 39/50\n",
      "11497/11497 [==============================] - 3s 277us/sample - loss: 1.0235\n",
      "Epoch 40/50\n",
      "11497/11497 [==============================] - 3s 299us/sample - loss: 1.0221\n",
      "Epoch 41/50\n",
      "11497/11497 [==============================] - 3s 287us/sample - loss: 1.0203\n",
      "Epoch 42/50\n",
      "11497/11497 [==============================] - 3s 276us/sample - loss: 1.0188\n",
      "Epoch 43/50\n",
      "11497/11497 [==============================] - 3s 258us/sample - loss: 1.0171\n",
      "Epoch 44/50\n",
      "11497/11497 [==============================] - 3s 271us/sample - loss: 1.0154\n",
      "Epoch 45/50\n",
      "11497/11497 [==============================] - 3s 278us/sample - loss: 1.0139\n",
      "Epoch 46/50\n",
      "11497/11497 [==============================] - 3s 296us/sample - loss: 1.0129\n",
      "Epoch 47/50\n",
      "11497/11497 [==============================] - 3s 269us/sample - loss: 1.0114\n",
      "Epoch 48/50\n",
      "11497/11497 [==============================] - 3s 270us/sample - loss: 1.0094\n",
      "Epoch 49/50\n",
      "11497/11497 [==============================] - 3s 270us/sample - loss: 1.0084\n",
      "Epoch 50/50\n",
      "11497/11497 [==============================] - 3s 285us/sample - loss: 1.0070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95f0168c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step will be to generate names, through a function `generate_names()`. \n",
    "\n",
    "To do so, you will have to give the output of the previous time step prediction as input to the next time step.\n",
    "\n",
    "You will have to use the method `predict_proba` of your model, as well as the method `numpy.random.choice`.\n",
    "\n",
    "Finally, use your function to generate some names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tkoriah\n",
      "\tarkun\n",
      "\teardan\n",
      "\tbetza\n",
      "\tfrelino\n",
      "\tolfida\n",
      "\tòrbera\n",
      "\tyothayna\n",
      "\tmahredit\n",
      "\tgrispalin\n",
      "\tianne\n",
      "\tkarja\n",
      "\tstilv\n",
      "\taorisos\n",
      "\tmalia\n",
      "\tevelami\n",
      "\tvarluba\n",
      "\tsinnel\n",
      "\thaires\n",
      "\trouslie\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement the function generate_names\n",
    "from generate import generate_n_names\n",
    "\n",
    "generate_n_names(20, maxlen, char_to_idx, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case this looks too complicated (indeed it is not simple), you can use the function `generate_n_names()` in the file `generate.py`. But first have a look at it and try to understand what it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have more time, you can try to improve the results by tuning your neural network hyperparameters.\n",
    "\n",
    "You can also use the original file, `Prenoms.csv`, and use only names from a given origin, to build a model more specific for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: This method can be applied to almost anything: you can generate music, shakespeare, lyrics... All it takes is to change the data preprocessing and adapt the dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
